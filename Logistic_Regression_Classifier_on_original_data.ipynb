{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Logistic Regression Classifier on original data.ipynb",
      "provenance": [],
      "mount_file_id": "14UGW8HzpQaVx_aHdLy362N8i-hhGN7px",
      "authorship_tag": "ABX9TyNLqGjyoIV6myZ4/YkpeKat",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/Aanisha/ACL_Abusive_Tamil_Comment_Classification/blob/main/Logistic_Regression_Classifier_on_original_data.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Logistic Regression on the dataset\n",
        "\n",
        "The experiment here uses the data without any sampling."
      ],
      "metadata": {
        "id": "w946rFQWXhNe"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 147,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "2zUtblVESanq",
        "outputId": "d1ee389f-82e4-4b76-9d31-10785f9c50a8"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: indic-nlp-library in /usr/local/lib/python3.7/dist-packages (0.81)\n",
            "Requirement already satisfied: morfessor in /usr/local/lib/python3.7/dist-packages (from indic-nlp-library) (2.0.6)\n",
            "Requirement already satisfied: sphinx-rtd-theme in /usr/local/lib/python3.7/dist-packages (from indic-nlp-library) (1.0.0)\n",
            "Requirement already satisfied: pandas in /usr/local/lib/python3.7/dist-packages (from indic-nlp-library) (1.3.5)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.7/dist-packages (from indic-nlp-library) (1.21.5)\n",
            "Requirement already satisfied: sphinx-argparse in /usr/local/lib/python3.7/dist-packages (from indic-nlp-library) (0.3.1)\n",
            "Requirement already satisfied: python-dateutil>=2.7.3 in /usr/local/lib/python3.7/dist-packages (from pandas->indic-nlp-library) (2.8.2)\n",
            "Requirement already satisfied: pytz>=2017.3 in /usr/local/lib/python3.7/dist-packages (from pandas->indic-nlp-library) (2018.9)\n",
            "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.7/dist-packages (from python-dateutil>=2.7.3->pandas->indic-nlp-library) (1.15.0)\n",
            "Requirement already satisfied: sphinx>=1.2.0 in /usr/local/lib/python3.7/dist-packages (from sphinx-argparse->indic-nlp-library) (1.8.6)\n",
            "Requirement already satisfied: packaging in /usr/local/lib/python3.7/dist-packages (from sphinx>=1.2.0->sphinx-argparse->indic-nlp-library) (21.3)\n",
            "Requirement already satisfied: alabaster<0.8,>=0.7 in /usr/local/lib/python3.7/dist-packages (from sphinx>=1.2.0->sphinx-argparse->indic-nlp-library) (0.7.12)\n",
            "Requirement already satisfied: setuptools in /usr/local/lib/python3.7/dist-packages (from sphinx>=1.2.0->sphinx-argparse->indic-nlp-library) (57.4.0)\n",
            "Requirement already satisfied: docutils<0.18,>=0.11 in /usr/local/lib/python3.7/dist-packages (from sphinx>=1.2.0->sphinx-argparse->indic-nlp-library) (0.17.1)\n",
            "Requirement already satisfied: imagesize in /usr/local/lib/python3.7/dist-packages (from sphinx>=1.2.0->sphinx-argparse->indic-nlp-library) (1.3.0)\n",
            "Requirement already satisfied: sphinxcontrib-websupport in /usr/local/lib/python3.7/dist-packages (from sphinx>=1.2.0->sphinx-argparse->indic-nlp-library) (1.2.4)\n",
            "Requirement already satisfied: snowballstemmer>=1.1 in /usr/local/lib/python3.7/dist-packages (from sphinx>=1.2.0->sphinx-argparse->indic-nlp-library) (2.2.0)\n",
            "Requirement already satisfied: Pygments>=2.0 in /usr/local/lib/python3.7/dist-packages (from sphinx>=1.2.0->sphinx-argparse->indic-nlp-library) (2.6.1)\n",
            "Requirement already satisfied: babel!=2.0,>=1.3 in /usr/local/lib/python3.7/dist-packages (from sphinx>=1.2.0->sphinx-argparse->indic-nlp-library) (2.9.1)\n",
            "Requirement already satisfied: requests>=2.0.0 in /usr/local/lib/python3.7/dist-packages (from sphinx>=1.2.0->sphinx-argparse->indic-nlp-library) (2.23.0)\n",
            "Requirement already satisfied: Jinja2>=2.3 in /usr/local/lib/python3.7/dist-packages (from sphinx>=1.2.0->sphinx-argparse->indic-nlp-library) (2.11.3)\n",
            "Requirement already satisfied: MarkupSafe>=0.23 in /usr/local/lib/python3.7/dist-packages (from Jinja2>=2.3->sphinx>=1.2.0->sphinx-argparse->indic-nlp-library) (2.0.1)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.7/dist-packages (from requests>=2.0.0->sphinx>=1.2.0->sphinx-argparse->indic-nlp-library) (2021.10.8)\n",
            "Requirement already satisfied: idna<3,>=2.5 in /usr/local/lib/python3.7/dist-packages (from requests>=2.0.0->sphinx>=1.2.0->sphinx-argparse->indic-nlp-library) (2.10)\n",
            "Requirement already satisfied: chardet<4,>=3.0.2 in /usr/local/lib/python3.7/dist-packages (from requests>=2.0.0->sphinx>=1.2.0->sphinx-argparse->indic-nlp-library) (3.0.4)\n",
            "Requirement already satisfied: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in /usr/local/lib/python3.7/dist-packages (from requests>=2.0.0->sphinx>=1.2.0->sphinx-argparse->indic-nlp-library) (1.24.3)\n",
            "Requirement already satisfied: pyparsing!=3.0.5,>=2.0.2 in /usr/local/lib/python3.7/dist-packages (from packaging->sphinx>=1.2.0->sphinx-argparse->indic-nlp-library) (3.0.7)\n",
            "Requirement already satisfied: sphinxcontrib-serializinghtml in /usr/local/lib/python3.7/dist-packages (from sphinxcontrib-websupport->sphinx>=1.2.0->sphinx-argparse->indic-nlp-library) (1.1.5)\n"
          ]
        }
      ],
      "source": [
        "# Downloading library\n",
        "\n",
        "!pip install indic-nlp-library"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd, numpy as np\n",
        "from sklearn.linear_model import LogisticRegression\n",
        "from sklearn.feature_extraction.text import TfidfVectorizer"
      ],
      "metadata": {
        "id": "CoyLxBvjTCli"
      },
      "execution_count": 148,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Preprocessing the data"
      ],
      "metadata": {
        "id": "VVkflqT8bRdj"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "train = pd.read_csv('/content/Tamil_train_data.csv')\n",
        "test = pd.read_csv('/content/Tamil_test_data.csv')\n",
        "valid = pd.read_csv('/content/Tamil_valid_data.csv')"
      ],
      "metadata": {
        "id": "egDs_caZTY8q"
      },
      "execution_count": 149,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "train = train[train.tag != 'Not-Tamil']"
      ],
      "metadata": {
        "id": "cdjAnJzmTyHI"
      },
      "execution_count": 150,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "tags = {\"tag\":     {'Hope-Speech':0, 'None-of-the-above':7, 'Homophobia':1, 'Misandry':2,\n",
        "       'Counter-speech':3, 'Misogyny':4, 'Xenophobia':5, 'Transphobic':6}}"
      ],
      "metadata": {
        "id": "6gxDAUsAUNnm"
      },
      "execution_count": 151,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "train = train.replace(tags)\n",
        "valid = valid.replace(tags)\n",
        "test = test.replace(tags)"
      ],
      "metadata": {
        "id": "RuE7vMtFUzkl"
      },
      "execution_count": 152,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "train = pd.concat([train,valid],axis=0)"
      ],
      "metadata": {
        "id": "qXEcRozRVmT_"
      },
      "execution_count": 153,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "train.shape"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "hY6Lh9snVtUk",
        "outputId": "9cb87ff8-4b96-4809-9df1-b993807a2637"
      },
      "execution_count": 154,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(10227, 2)"
            ]
          },
          "metadata": {},
          "execution_count": 154
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#train = train.drop(train[train['tag'] == 7].sample(frac=0.4).index)"
      ],
      "metadata": {
        "id": "kJrc_HuytwFe"
      },
      "execution_count": 155,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "test_labels = pd.read_csv(\"/content/Tamil_test_labels_data.csv\")\n",
        "\n",
        "test_labels = test_labels.replace(tags)\n",
        "test_labels = pd.merge(test_labels, test, on=['comments'])\n",
        "test_labels = test_labels.dropna()"
      ],
      "metadata": {
        "id": "X8LFjK5iyUH1"
      },
      "execution_count": 156,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "gt = []\n",
        "co = []\n",
        "for com in range(len(list(test_labels['comments']))):\n",
        "  if test_labels['comments'][com] in list(test['comments']):\n",
        "\n",
        "    gt.append(test_labels['tag'][com])\n",
        "    co.append(test_labels['comments'][com])"
      ],
      "metadata": {
        "id": "f9J99E3IyZf_"
      },
      "execution_count": 157,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "len(co)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "701vIHx4yjCF",
        "outputId": "0bcd61f6-12ad-4c17-8eb3-023f3ade16f4"
      },
      "execution_count": 158,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "2555"
            ]
          },
          "metadata": {},
          "execution_count": 158
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def oversample(df):\n",
        "  classes = [4,0,1,6,5]\n",
        "  most = 250\n",
        "  classes_list = []\n",
        "  for key in classes:\n",
        "      classes_list.append(df[df['tag'] == key]) \n",
        "  classes_sample = []\n",
        "  for i in range(len(classes_list)):\n",
        "      classes_sample.append(classes_list[i].sample(most, replace=True))\n",
        "  df_maybe = pd.concat(classes_sample)\n",
        "  final_df = pd.concat([df_maybe,df], axis=0)\n",
        "  final_df = final_df.reset_index(drop=True)\n",
        "  return final_df"
      ],
      "metadata": {
        "id": "vEKU-mQct2Vy"
      },
      "execution_count": 159,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#train = oversample(train)"
      ],
      "metadata": {
        "id": "Jrl2mv3ot7YK"
      },
      "execution_count": 160,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Tokenization of the train data"
      ],
      "metadata": {
        "id": "rSnr5z6MbH1o"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import re, string\n",
        "from indicnlp.tokenize import indic_tokenize\n",
        "def tokenize(s): return indic_tokenize.trivial_tokenize(s)"
      ],
      "metadata": {
        "id": "ezKOgeFwU3gy"
      },
      "execution_count": 161,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "n = train.shape[0]\n",
        "\n",
        "vec = TfidfVectorizer(ngram_range=(1,2), tokenizer=tokenize,\n",
        "                      strip_accents='unicode', use_idf=1,\n",
        "               smooth_idf=1, sublinear_tf=1)\n",
        "\n",
        "\n",
        "trn_term_doc = vec.fit_transform(train['comments'])\n",
        "test_term_doc = vec.transform(co)"
      ],
      "metadata": {
        "id": "7pysTr8JVHj8"
      },
      "execution_count": 162,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Training the model on the training data"
      ],
      "metadata": {
        "id": "OqkZ_XE2a6he"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "label_cols = ['tag']"
      ],
      "metadata": {
        "id": "qzCweAheXmi6"
      },
      "execution_count": 163,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "x = trn_term_doc\n",
        "test_x = test_term_doc"
      ],
      "metadata": {
        "id": "GvxdUq6JVaLN"
      },
      "execution_count": 164,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def pr(y_i, y):\n",
        "    p = x[y==y_i].sum(0)\n",
        "    return (p+1) / ((y==y_i).sum()+1)"
      ],
      "metadata": {
        "id": "oQEV7lVzVSzL"
      },
      "execution_count": 165,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import sklearn.svm as svm\n",
        "import sklearn.ensemble\n",
        "\n",
        "def get_mdl(y):\n",
        "    y = y.values\n",
        "    r = np.log(pr(1,y) / pr(0,y))\n",
        "    m = LogisticRegression(solver='newton-cg')\n",
        "    x_nb = x.multiply(r)\n",
        "    return m.fit(x_nb, y), r"
      ],
      "metadata": {
        "id": "R9d49XB_WiDm"
      },
      "execution_count": 166,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "preds = np.zeros((len(test), len(label_cols)))\n",
        "\n",
        "for i, j in enumerate(label_cols):\n",
        "    print('fit', j)\n",
        "    m,r = get_mdl(train[j])\n",
        "    preds = m.predict(test_x.multiply(r))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ONkHmHTuWk_f",
        "outputId": "5f70d0d0-9b1d-4852-ff73-cf2095eb361d"
      },
      "execution_count": 167,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "fit tag\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "len(preds)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Srov1HIuu5jp",
        "outputId": "fbe01297-28c6-4247-8864-8149e6f0482e"
      },
      "execution_count": 168,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "2555"
            ]
          },
          "metadata": {},
          "execution_count": 168
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Preparing the test data"
      ],
      "metadata": {
        "id": "fv9cbwAeanmg"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "test_labels = pd.read_csv(\"/content/Tamil_test_labels_data.csv\")"
      ],
      "metadata": {
        "id": "Rwy8XaVPXpDn"
      },
      "execution_count": 169,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "test_labels.shape"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "g3r5dd-dbixZ",
        "outputId": "8ac49c8c-b7a6-46cf-aca7-6bbcaf3d7458"
      },
      "execution_count": 170,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(2559, 2)"
            ]
          },
          "metadata": {},
          "execution_count": 170
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "test_labels = test_labels.replace(tags)"
      ],
      "metadata": {
        "id": "5av1jK3GX04z"
      },
      "execution_count": 171,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "test_labels = test_labels.dropna()"
      ],
      "metadata": {
        "id": "N4A_xXtRj4Qv"
      },
      "execution_count": 172,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "test_labels.shape"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "rgci5vyzbpyJ",
        "outputId": "301715bc-5341-4118-9f74-4c187150c33f"
      },
      "execution_count": 173,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(2556, 2)"
            ]
          },
          "metadata": {},
          "execution_count": 173
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Testing the model using unseen test data"
      ],
      "metadata": {
        "id": "Zg8TuNq-atqE"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import sklearn\n",
        "\n",
        "print(sklearn.metrics.classification_report(gt, preds))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "fFyPVQ5NYQDR",
        "outputId": "4c5250f2-a72b-4ef6-ae85-58ce215d2d23"
      },
      "execution_count": 174,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.17      0.01      0.02        95\n",
            "           1       0.71      0.16      0.26        64\n",
            "           2       0.71      0.37      0.49       419\n",
            "           3       0.00      0.00      0.00       135\n",
            "           4       0.71      0.05      0.09       105\n",
            "           5       1.00      0.01      0.02       120\n",
            "           6       0.00      0.00      0.00        60\n",
            "           7       0.65      0.97      0.78      1557\n",
            "\n",
            "    accuracy                           0.66      2555\n",
            "   macro avg       0.50      0.20      0.21      2555\n",
            "weighted avg       0.62      0.66      0.57      2555\n",
            "\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/sklearn/metrics/_classification.py:1318: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
            "  _warn_prf(average, modifier, msg_start, len(result))\n",
            "/usr/local/lib/python3.7/dist-packages/sklearn/metrics/_classification.py:1318: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
            "  _warn_prf(average, modifier, msg_start, len(result))\n",
            "/usr/local/lib/python3.7/dist-packages/sklearn/metrics/_classification.py:1318: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
            "  _warn_prf(average, modifier, msg_start, len(result))\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        ""
      ],
      "metadata": {
        "id": "E0ZBmymLYUfH"
      },
      "execution_count": 118,
      "outputs": []
    }
  ]
}